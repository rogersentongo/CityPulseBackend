#!/bin/bash

# =============================================================================
# CityPulse Ollama Setup Script
# =============================================================================
# This script installs Ollama and downloads the required models for CityPulse
# Run this script to set up local AI inference capabilities

set -e  # Exit on any error

echo "ğŸ¦™ Setting up Ollama for CityPulse..."
echo "This will install Ollama and download the required models (~3-4 GB total)"
echo ""

# Check if running on macOS or Linux
if [[ "$OSTYPE" == "darwin"* ]]; then
    PLATFORM="macOS"
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    PLATFORM="Linux"
else
    echo "âŒ Unsupported platform: $OSTYPE"
    echo "Please install Ollama manually from https://ollama.ai"
    exit 1
fi

echo "ğŸ“± Detected platform: $PLATFORM"

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Install Ollama if not already installed
if command_exists ollama; then
    echo "âœ… Ollama is already installed"
    ollama --version
else
    echo "ğŸ“¥ Installing Ollama..."

    if [[ "$PLATFORM" == "macOS" ]]; then
        # Check if Homebrew is available
        if command_exists brew; then
            echo "ğŸº Installing via Homebrew..."
            brew install ollama
        else
            echo "ğŸŒ Installing via curl..."
            curl -fsSL https://ollama.ai/install.sh | sh
        fi
    else
        # Linux installation
        echo "ğŸŒ Installing via curl..."
        curl -fsSL https://ollama.ai/install.sh | sh
    fi

    echo "âœ… Ollama installed successfully"
fi

echo ""
echo "ğŸš€ Starting Ollama service..."

# Start Ollama service in background
if [[ "$PLATFORM" == "macOS" ]]; then
    # On macOS, check if Ollama is running
    if ! pgrep -f "ollama" > /dev/null; then
        echo "Starting Ollama server..."
        ollama serve &
        OLLAMA_PID=$!
        echo "Ollama server started with PID: $OLLAMA_PID"
    else
        echo "âœ… Ollama server is already running"
    fi
else
    # On Linux, try to start as service or background process
    if systemctl is-active --quiet ollama 2>/dev/null; then
        echo "âœ… Ollama service is already running"
    elif command_exists systemctl; then
        echo "Starting Ollama service..."
        sudo systemctl start ollama
        sudo systemctl enable ollama
    else
        echo "Starting Ollama server in background..."
        ollama serve &
        OLLAMA_PID=$!
        echo "Ollama server started with PID: $OLLAMA_PID"
    fi
fi

# Wait for Ollama to be ready
echo "â³ Waiting for Ollama to be ready..."
sleep 3

# Check if Ollama is responding
for i in {1..10}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama is ready!"
        break
    else
        echo "Waiting for Ollama... (attempt $i/10)"
        sleep 2
    fi

    if [ $i -eq 10 ]; then
        echo "âŒ Ollama failed to start. Please check the installation."
        exit 1
    fi
done

echo ""
echo "ğŸ“¦ Downloading required models..."

# Download LLM model for text generation
echo "ğŸ§  Downloading LLM model: llama3.2:3b (~2GB)..."
ollama pull llama3.2:3b

# Download embedding model
echo "ğŸ”¢ Downloading embedding model: nomic-embed-text (~274MB)..."
ollama pull nomic-embed-text

echo ""
echo "ğŸ‰ Setup complete! Ollama is ready for CityPulse."
echo ""
echo "ğŸ“‹ Installed models:"
ollama list

echo ""
echo "ğŸ”— Ollama is running at: http://localhost:11434"
echo "ğŸ“ Your CityPulse backend is now configured to use local AI inference!"
echo ""
echo "ğŸ’¡ To verify everything is working:"
echo "   uv run python -c \"import ollama; print('Ollama client works!')\""
echo ""
echo "ğŸš€ You can now start your CityPulse backend with:"
echo "   uv run uvicorn app.main:app --reload"